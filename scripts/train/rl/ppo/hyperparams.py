
# ------ total training time -------
train_timesteps = {"n_parity": 150_000_000,
                   "n_parity_only_n": 150_000_000,
                   "simple_alu": 450_000_000,
                   "halfcheetah": 50_000_000,
                   "ant": 150_000_000,
                   "locomotion": 1_000_000,
                   "locomotion_with_obstacles": 150_000_000,
                   "deceptive_maze_easy": 150_000_000,
                   "maze_with_stepping_stones": 150_000_000,
                   }

# ------ PPO hyperparams for brax implementation -------
hyperparams = {
    "n_parity": {"reward_scaling": 1,
                 "unroll_length": 1,
                 "num_updates_per_batch": 4,
                 "discounting": 1.0,
                 "learning_rate": 3e-4,
                 "num_minibatches": 32,
                 "normalize_observations": False,
                 "num_envs": 4096,
                 "num_evals": 50,
                 "batch_size": 2048,
                 "entropy_cost": 1e-2,
                 "action_repeat": 1,
                 },
    "n_parity_only_n": {"reward_scaling": 1,
                 "unroll_length": 1,
                 "num_updates_per_batch": 4,
                 "discounting": 1.0,
                 "learning_rate": 3e-4,
                 "num_minibatches": 32,
                 "normalize_observations": False,
                 "num_envs": 4096,
                 "num_evals": 50,
                 "batch_size": 2048,
                 "entropy_cost": 1e-2,
                 "action_repeat": 1,
                 },
    

    "simple_alu": {"reward_scaling": 1,
                   "unroll_length": 1,
                   "num_updates_per_batch": 4,
                   "discounting": 1.0,
                   "learning_rate": 3e-4,
                   "num_minibatches": 32,
                   "normalize_observations": False,
                   "num_envs": 4096,
                   "num_evals": 50,
                   "batch_size": 2048,
                   "entropy_cost": 1e-2,
                   "action_repeat": 1,
                 },
    "halfcheetah": {"reward_scaling": 1,
                   "unroll_length": 20,
                   "num_updates_per_batch": 8,
                   "discounting": 0.95,
                   "learning_rate": 3e-4,
                   "num_minibatches": 32,
                   "normalize_observations": True,
                   "num_envs": 2048,
                   "num_evals": 20,
                   "batch_size": 512,
                   "entropy_cost": 1e-2,
                   "action_repeat": 1,
                   },
        "ant": {"reward_scaling": 10,
                   "unroll_length": 20,
                   "num_updates_per_batch": 8,
                   "discounting": 0.95,
                   "learning_rate": 3e-4,
                   "num_minibatches": 32,
                   "normalize_observations": True,
                   "num_envs": 2048,
                   "num_evals": 20,
                   "batch_size": 512,
                   "entropy_cost": 1e-2,
                   "action_repeat": 1,
                   },
      "locomotion": {"reward_scaling": 10,
                  "unroll_length": 20,
                  "num_updates_per_batch": 8,
                  "discounting": 0.95,
                  "learning_rate": 3e-4,
                  "num_minibatches": 32,
                  "normalize_observations": True,
                  "num_envs": 128,
                  "num_evals": 20,
                  "batch_size": 512,
                  "entropy_cost": 1e-2,
                  "action_repeat": 1,
                  },
       "locomotion_with_obstacles": {"reward_scaling": 10,
                  "unroll_length": 20,
                  "num_updates_per_batch": 8,
                  "discounting": 0.95,
                  "learning_rate": 3e-4,
                  "num_minibatches": 32,
                  "normalize_observations": True,
                  "num_envs": 128,
                  "num_evals": 20,
                  "batch_size": 512,
                  "entropy_cost": 1e-2,
                  "action_repeat": 1,
                  },
       "deceptive_maze_easy": {"reward_scaling": 10,
                  "unroll_length": 20,
                  "num_updates_per_batch": 8,
                  "discounting": 0.95,
                  "learning_rate": 3e-4,
                  "num_minibatches": 32,
                  "normalize_observations": True,
                  "num_envs": 128,
                  "num_evals": 20,
                  "batch_size": 512,
                  "entropy_cost": 1e-2,
                  "action_repeat": 1,
                  },
       "maze_with_stepping_stones": {"reward_scaling": 10,
                  "unroll_length": 20,
                  "num_updates_per_batch": 8,
                  "discounting": 0.95,
                  "learning_rate": 3e-4,
                  "num_minibatches": 32,
                  "normalize_observations": True,
                  "num_envs": 128,
                  "num_evals": 20,
                  "batch_size": 512,
                  "entropy_cost": 1e-2,
                  "action_repeat": 1,
                  },
}

# ------ neural network architecture for policy network. just MLPs ------
arch = {"n_parity": {"num_layers": 6,
                     "num_neurons": 4},
        "n_parity_only_n": {"num_layers": 6,
                     "num_neurons": 4},
        "simple_alu": {"num_layers": 6,
                     "num_neurons": 4},
          "halfcheetah": {"num_layers": 4,
                      "num_neurons": 32},
          "ant": {"num_layers": 4,
                      "num_neurons": 32},
        ("locomotion", "halfcheetah"): {"num_layers": 4,
                     "num_neurons": 32},
        ("locomotion", "ant"): {"num_layers": 4,
                     "num_neurons": 32},
        ("locomotion_with_obstacles", "halfcheetah"): {"num_layers": 4,
                     "num_neurons": 32},
        ("deceptive_maze_easy", "ant"): {"num_layers": 4,
                     "num_neurons": 32},
        ("deceptive_maze_easy", "discrete_fish"): {"num_layers": 4,
                     "num_neurons": 32},
        ("maze_with_stepping_stones", "discrete_fish"): {"num_layers": 4,
                     "num_neurons": 32}}



